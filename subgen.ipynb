{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d0845fe4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-05T11:40:12.767587Z",
     "iopub.status.busy": "2025-07-05T11:40:12.766582Z",
     "iopub.status.idle": "2025-07-05T11:40:12.772514Z",
     "shell.execute_reply": "2025-07-05T11:40:12.771453Z"
    },
    "papermill": {
     "duration": 0.012958,
     "end_time": "2025-07-05T11:40:12.774232",
     "exception": false,
     "start_time": "2025-07-05T11:40:12.761274",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install pymupdf\n",
    "# !pip install pdfminer.six\n",
    "# \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "31fa2941",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-05T11:40:12.782273Z",
     "iopub.status.busy": "2025-07-05T11:40:12.781515Z",
     "iopub.status.idle": "2025-07-05T11:40:42.910717Z",
     "shell.execute_reply": "2025-07-05T11:40:42.909196Z"
    },
    "papermill": {
     "duration": 30.135197,
     "end_time": "2025-07-05T11:40:42.912638",
     "exception": false,
     "start_time": "2025-07-05T11:40:12.777441",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“¥ Loading labels...\n",
      "ðŸ” Extracting training samples...\n",
      "âœ… Loaded 1028 samples. Label distribution: Counter({0: 758, 1: 270})\n",
      "ðŸ”  Vectorizing text with TF-IDF...\n",
      "\n",
      "ðŸ“Š Evaluation Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.97      0.95       152\n",
      "           1       0.91      0.76      0.83        54\n",
      "\n",
      "    accuracy                           0.92       206\n",
      "   macro avg       0.92      0.87      0.89       206\n",
      "weighted avg       0.92      0.92      0.91       206\n",
      "\n",
      "Best threshold: 0.46, F1: 0.8491\n",
      "ðŸ’¾ Model and vectorizer saved.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from lxml import etree\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, precision_recall_curve\n",
    "import joblib\n",
    "\n",
    "# === Paths ===\n",
    "LABELS_CSV = \"/kaggle/input/make-data-count-finding-data-references/train_labels.csv\"\n",
    "XML_DIR = \"/kaggle/input/make-data-count-finding-data-references/train/XML/\"\n",
    "PDF_DIR = \"/kaggle/input/make-data-count-finding-data-references/train/PDF/\"\n",
    "MODEL_PATH = \"clf_model.joblib\"\n",
    "VECTORIZER_PATH = \"vectorizer.joblib\"\n",
    "BERT_MODEL = 'all-MiniLM-L6-v2'  # unused now, kept for reference\n",
    "\n",
    "# === Load Labels ===\n",
    "print(\"ðŸ“¥ Loading labels...\")\n",
    "labels_df = pd.read_csv(LABELS_CSV)\n",
    "\n",
    "# === Utilities ===\n",
    "def extract_text_from_xml(file_path):\n",
    "    try:\n",
    "        tree = etree.parse(file_path)\n",
    "        return \" \".join(tree.xpath('//text()'))\n",
    "    except Exception:\n",
    "        return \"\"\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    try:\n",
    "        with open(pdf_path, \"rb\") as f:\n",
    "            return f.read().decode(\"latin1\", errors=\"ignore\")\n",
    "    except:\n",
    "        return \"\"\n",
    "\n",
    "def normalize_doi(doi):\n",
    "    doi = doi.strip().lower()\n",
    "    if doi.startswith(\"https://doi.org/\"):\n",
    "        return doi\n",
    "    if doi.startswith(\"doi:\"):\n",
    "        doi = doi[4:]\n",
    "    if doi.startswith(\"10.\"):\n",
    "        return \"https://doi.org/\" + doi\n",
    "    return doi\n",
    "\n",
    "def extract_references(text):\n",
    "    dois = re.findall(r'\\b10\\.\\d{4,9}/[-._;()/:a-z0-9]+', text, flags=re.I)\n",
    "    accessions = re.findall(r'\\b(GSE\\d+|E-[A-Z]+-\\d+|PRJ[EDNA]\\d+|PDB\\s*\\w+)\\b', text, flags=re.I)\n",
    "    references = set()\n",
    "    for doi in dois:\n",
    "        references.add(normalize_doi(doi))\n",
    "    for acc in accessions:\n",
    "        references.add(acc.replace(\" \", \"\").upper())\n",
    "    return references\n",
    "\n",
    "# === Data Prep ===\n",
    "print(\"ðŸ” Extracting training samples...\")\n",
    "contexts, labels = [], []\n",
    "\n",
    "for _, row in labels_df.iterrows():\n",
    "    article_id, dataset_id, label_type = row['article_id'], row['dataset_id'], row['type']\n",
    "    xml_file = os.path.join(XML_DIR, article_id + \".xml\")\n",
    "    pdf_file = os.path.join(PDF_DIR, article_id + \".pdf\")\n",
    "\n",
    "    full_text = \"\"\n",
    "    if os.path.exists(xml_file):\n",
    "        full_text = extract_text_from_xml(xml_file)\n",
    "    elif os.path.exists(pdf_file):\n",
    "        full_text = extract_text_from_pdf(pdf_file)\n",
    "\n",
    "    if not full_text:\n",
    "        continue\n",
    "\n",
    "    match = re.search(re.escape(dataset_id), full_text, flags=re.I)\n",
    "    if match:\n",
    "        snippet = full_text[max(match.start() - 100, 0):match.end() + 100].replace('\\n', ' ')\n",
    "    else:\n",
    "        snippet = full_text[:150]\n",
    "\n",
    "    contexts.append(snippet)\n",
    "    labels.append(1 if label_type.lower() == \"primary\" else 0)\n",
    "\n",
    "print(f\"âœ… Loaded {len(contexts)} samples. Label distribution: {Counter(labels)}\")\n",
    "\n",
    "# === Feature Engineering ===\n",
    "print(\"ðŸ”  Vectorizing text with TF-IDF...\")\n",
    "vectorizer = TfidfVectorizer(max_features=10000)\n",
    "X = vectorizer.fit_transform(contexts)\n",
    "y = np.array(labels)\n",
    "\n",
    "# === Train/Test Split ===\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, stratify=y, test_size=0.2, random_state=42)\n",
    "\n",
    "# === Model Training ===\n",
    "clf = LogisticRegression(max_iter=1000, class_weight='balanced')\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# === Evaluation ===\n",
    "y_pred_proba = clf.predict_proba(X_val)[:, 1]\n",
    "precisions, recalls, thresholds = precision_recall_curve(y_val, y_pred_proba)\n",
    "f1_scores = 2 * (precisions * recalls) / (precisions + recalls + 1e-6)\n",
    "best_idx = np.argmax(f1_scores)\n",
    "\n",
    "print(\"\\nðŸ“Š Evaluation Report:\")\n",
    "print(classification_report(y_val, clf.predict(X_val)))\n",
    "print(f\"Best threshold: {thresholds[best_idx]:.2f}, F1: {f1_scores[best_idx]:.4f}\")\n",
    "\n",
    "# === Save Artifacts ===\n",
    "joblib.dump(clf, MODEL_PATH)\n",
    "joblib.dump(vectorizer, VECTORIZER_PATH)\n",
    "print(\"ðŸ’¾ Model and vectorizer saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "158d2174",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-05T11:40:42.921137Z",
     "iopub.status.busy": "2025-07-05T11:40:42.920043Z",
     "iopub.status.idle": "2025-07-05T11:40:45.366814Z",
     "shell.execute_reply": "2025-07-05T11:40:45.365341Z"
    },
    "papermill": {
     "duration": 2.453136,
     "end_time": "2025-07-05T11:40:45.368906",
     "exception": false,
     "start_time": "2025-07-05T11:40:42.915770",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ” Running inference on test set...\n",
      "âœ… Submission file created as submission.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import joblib\n",
    "\n",
    "from lxml import etree\n",
    "\n",
    "# === Paths ===\n",
    "TEST_XML_DIR = \"/kaggle/input/make-data-count-finding-data-references/test/XML\"\n",
    "TEST_PDF_DIR = \"/kaggle/input/make-data-count-finding-data-references/test/PDF\"\n",
    "MODEL_PATH = \"clf_model.joblib\"\n",
    "VECTORIZER_PATH = \"vectorizer.joblib\"\n",
    "SUBMISSION_FILE = \"submission.csv\"\n",
    "BEST_THRESH = 0.46  # Use same threshold from training\n",
    "\n",
    "# === Load Artifacts ===\n",
    "clf = joblib.load(MODEL_PATH)\n",
    "vectorizer = joblib.load(VECTORIZER_PATH)\n",
    "\n",
    "# === Utilities ===\n",
    "def extract_text_from_xml(file_path):\n",
    "    try:\n",
    "        tree = etree.parse(file_path)\n",
    "        return \" \".join(tree.xpath('//text()'))\n",
    "    except:\n",
    "        return \"\"\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    try:\n",
    "        with open(pdf_path, \"rb\") as f:\n",
    "            return f.read().decode(\"latin1\", errors=\"ignore\")\n",
    "    except:\n",
    "        return \"\"\n",
    "\n",
    "def extract_text(article_id):\n",
    "    xml_path = os.path.join(TEST_XML_DIR, article_id + \".xml\")\n",
    "    pdf_path = os.path.join(TEST_PDF_DIR, article_id + \".pdf\")\n",
    "    if os.path.exists(xml_path):\n",
    "        return extract_text_from_xml(xml_path)\n",
    "    elif os.path.exists(pdf_path):\n",
    "        return extract_text_from_pdf(pdf_path)\n",
    "    return \"\"\n",
    "\n",
    "def normalize_doi(doi):\n",
    "    doi = doi.strip().lower()\n",
    "    if doi.startswith(\"https://doi.org/\"):\n",
    "        return doi\n",
    "    if doi.startswith(\"doi:\"):\n",
    "        doi = doi[4:]\n",
    "    if doi.startswith(\"10.\"):\n",
    "        return \"https://doi.org/\" + doi\n",
    "    return doi\n",
    "\n",
    "def extract_references(text):\n",
    "    dois = re.findall(r'\\b10\\.\\d{4,9}/[-._;()/:a-z0-9]+', text, flags=re.I)\n",
    "    accessions = re.findall(r'\\b(GSE\\d+|E-[A-Z]+-\\d+|PRJ[EDNA]\\d+|PDB\\s*\\w+)\\b', text, flags=re.I)\n",
    "    references = set()\n",
    "    for doi in dois:\n",
    "        references.add(normalize_doi(doi))\n",
    "    for acc in accessions:\n",
    "        references.add(acc.replace(\" \", \"\").upper())\n",
    "    return references\n",
    "\n",
    "# === Inference ===\n",
    "print(\"ðŸ” Running inference on test set...\")\n",
    "submission_rows = []\n",
    "\n",
    "for fname in os.listdir(TEST_XML_DIR):\n",
    "    if not fname.endswith(\".xml\"):\n",
    "        continue\n",
    "    article_id = fname.replace(\".xml\", \"\")\n",
    "    text = extract_text(article_id)\n",
    "    if not text:\n",
    "        continue\n",
    "\n",
    "    refs = extract_references(text)\n",
    "    for dataset_id in refs:\n",
    "        match = re.search(re.escape(dataset_id), text, flags=re.I)\n",
    "        if match:\n",
    "            snippet = text[max(match.start() - 100, 0):match.end() + 100].replace(\"\\n\", \" \")\n",
    "        else:\n",
    "            snippet = text[:150]\n",
    "\n",
    "        features = vectorizer.transform([snippet])\n",
    "        prob = clf.predict_proba(features)[0, 1]\n",
    "        citation_type = \"Primary\" if prob >= BEST_THRESH else \"Secondary\"\n",
    "\n",
    "        submission_rows.append({\n",
    "            \"article_id\": article_id,\n",
    "            \"dataset_id\": dataset_id,\n",
    "            \"type\": citation_type\n",
    "        })\n",
    "\n",
    "# === Save Submission ===\n",
    "submission_df = pd.DataFrame(submission_rows).drop_duplicates()\n",
    "submission_df.insert(0, \"row_id\", range(len(submission_df)))\n",
    "submission_df.to_csv(SUBMISSION_FILE, index=False)\n",
    "print(\"âœ… Submission file created as submission.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cef7ce22",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-05T11:40:45.377733Z",
     "iopub.status.busy": "2025-07-05T11:40:45.377349Z",
     "iopub.status.idle": "2025-07-05T11:40:45.383817Z",
     "shell.execute_reply": "2025-07-05T11:40:45.382732Z"
    },
    "papermill": {
     "duration": 0.012965,
     "end_time": "2025-07-05T11:40:45.385586",
     "exception": false,
     "start_time": "2025-07-05T11:40:45.372621",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "# import re\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# from lxml import etree\n",
    "# from scipy.sparse import hstack\n",
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "# import joblib\n",
    "\n",
    "# # === Configuration ===\n",
    "# TEST_XML_FOLDER = \"/kaggle/input/make-data-count-finding-data-references/test/XML\"  # Change to your test XML path\n",
    "# OUTPUT_CSV = \"submission.csv\"\n",
    "# MODEL_PATH = \"/kaggle/working/clf_model.joblib\"\n",
    "# VECTORIZER_PATH = \"/kaggle/working/vectorizer.joblib\"\n",
    "# BEST_THRESHOLD = 0.54  # from training\n",
    "\n",
    "# # === Load Artifacts ===\n",
    "# clf = joblib.load(MODEL_PATH)\n",
    "# vectorizer = joblib.load(VECTORIZER_PATH)\n",
    "\n",
    "# def extract_text_from_xml(file_path):\n",
    "#     try:\n",
    "#         tree = etree.parse(file_path)\n",
    "#         return \" \".join(tree.xpath('//text()'))\n",
    "#     except Exception:\n",
    "#         return \"\"\n",
    "\n",
    "# def normalize_doi(doi):\n",
    "#     doi = doi.strip().lower()\n",
    "#     if doi.startswith(\"https://doi.org/\"):\n",
    "#         return doi\n",
    "#     if doi.startswith(\"doi:\"):\n",
    "#         doi = doi[4:]\n",
    "#     if doi.startswith(\"10.\"):\n",
    "#         return \"https://doi.org/\" + doi\n",
    "#     return doi\n",
    "\n",
    "# def extract_references(text):\n",
    "#     dois = re.findall(r'\\b10\\.\\d{4,9}/[-._;()/:a-z0-9]+', text, flags=re.I)\n",
    "#     accessions = re.findall(r'\\b(GSE\\d+|E-[A-Z]+-\\d+|PRJ[EDNA]\\d+|PDB\\s*\\w+)\\b', text, flags=re.I)\n",
    "#     references = set()\n",
    "\n",
    "#     for doi in dois:\n",
    "#         doi_full = normalize_doi(doi)\n",
    "#         references.add(doi_full)\n",
    "\n",
    "#     for acc in accessions:\n",
    "#         acc_clean = acc.replace(\" \", \"\").upper()\n",
    "#         references.add(acc_clean)\n",
    "\n",
    "#     return list(references)\n",
    "\n",
    "# def keyword_feature(text):\n",
    "#     keywords = ['this study', 'generated', 'deposited', 'we used', 'available at']\n",
    "#     return int(any(k in text.lower() for k in keywords))\n",
    "\n",
    "# # === Inference ===\n",
    "# submission_rows = []\n",
    "\n",
    "# for file_name in os.listdir(TEST_XML_FOLDER):\n",
    "#     if not file_name.endswith(\".xml\"):\n",
    "#         continue\n",
    "\n",
    "#     article_id = file_name.replace(\".xml\", \"\")\n",
    "#     xml_path = os.path.join(TEST_XML_FOLDER, file_name)\n",
    "#     text = extract_text_from_xml(xml_path)\n",
    "#     refs = extract_references(text)\n",
    "\n",
    "#     for dataset_id in refs:\n",
    "#         match = re.search(re.escape(dataset_id), text, flags=re.I)\n",
    "#         snippet = text[max(match.start() - 100, 0):match.end() + 100] if match else text[:150]\n",
    "#         snippet = snippet.replace('\\n', ' ')\n",
    "\n",
    "#         x_tfidf = vectorizer.transform([snippet])\n",
    "#         x_kw = np.array([keyword_feature(snippet)]).reshape(-1, 1)\n",
    "#         x_feat = hstack([x_tfidf, x_kw])\n",
    "\n",
    "#         prob = clf.predict_proba(x_feat)[:, 1][0]\n",
    "#         pred_type = \"Primary\" if prob >= BEST_THRESHOLD else \"Secondary\"\n",
    "\n",
    "#         submission_rows.append({\n",
    "#             \"article_id\": article_id,\n",
    "#             \"dataset_id\": dataset_id,\n",
    "#             \"type\": pred_type\n",
    "#         })\n",
    "\n",
    "# # === Save Submission ===\n",
    "# submission_df = pd.DataFrame(submission_rows).drop_duplicates()\n",
    "# submission_df.insert(0, \"row_id\", range(len(submission_df)))\n",
    "# submission_df.to_csv(OUTPUT_CSV, index=False)\n",
    "\n",
    "# print(f\"âœ… Submission file saved to: {OUTPUT_CSV}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "79594301",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-05T11:40:45.393268Z",
     "iopub.status.busy": "2025-07-05T11:40:45.392856Z",
     "iopub.status.idle": "2025-07-05T11:40:45.397652Z",
     "shell.execute_reply": "2025-07-05T11:40:45.396419Z"
    },
    "papermill": {
     "duration": 0.010659,
     "end_time": "2025-07-05T11:40:45.399464",
     "exception": false,
     "start_time": "2025-07-05T11:40:45.388805",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# pip install pymupdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d1c68a9a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-05T11:40:45.408330Z",
     "iopub.status.busy": "2025-07-05T11:40:45.407472Z",
     "iopub.status.idle": "2025-07-05T11:40:45.414530Z",
     "shell.execute_reply": "2025-07-05T11:40:45.413453Z"
    },
    "papermill": {
     "duration": 0.013708,
     "end_time": "2025-07-05T11:40:45.416598",
     "exception": false,
     "start_time": "2025-07-05T11:40:45.402890",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "# import re\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# import joblib\n",
    "# from lxml import etree\n",
    "# from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# # === Paths ===\n",
    "# TEST_XML_DIR = \"/kaggle/input/make-data-count-finding-data-references/test/XML\"\n",
    "# TEST_PDF_DIR = \"/kaggle/input/make-data-count-finding-data-references/test/PDF\"\n",
    "# SUBMISSION_FILE = \"submission.csv\"\n",
    "# MODEL_PATH = \"clf_model.joblib\"\n",
    "# BERT_MODEL = \"all-MiniLM-L6-v2\"\n",
    "\n",
    "# # === Load Model ===\n",
    "# clf = joblib.load(MODEL_PATH)\n",
    "# bert = SentenceTransformer(BERT_MODEL)\n",
    "\n",
    "# # === Threshold ===\n",
    "# BEST_THRESH = 0.56  # From training script\n",
    "\n",
    "# # === Utility Functions ===\n",
    "# def extract_text_from_xml(file_path):\n",
    "#     try:\n",
    "#         tree = etree.parse(file_path)\n",
    "#         return \" \".join(tree.xpath('//text()'))\n",
    "#     except:\n",
    "#         return \"\"\n",
    "\n",
    "# def extract_text_from_pdf(pdf_path):\n",
    "#     try:\n",
    "#         with open(pdf_path, \"rb\") as f:\n",
    "#             return f.read().decode(\"latin1\", errors=\"ignore\")\n",
    "#     except:\n",
    "#         return \"\"\n",
    "\n",
    "# def extract_text(article_id):\n",
    "#     xml_path = os.path.join(TEST_XML_DIR, article_id + \".xml\")\n",
    "#     pdf_path = os.path.join(TEST_PDF_DIR, article_id + \".pdf\")\n",
    "#     if os.path.exists(xml_path):\n",
    "#         return extract_text_from_xml(xml_path)\n",
    "#     elif os.path.exists(pdf_path):\n",
    "#         return extract_text_from_pdf(pdf_path)\n",
    "#     return \"\"\n",
    "\n",
    "# def normalize_doi(doi):\n",
    "#     doi = doi.strip().lower()\n",
    "#     if doi.startswith(\"https://doi.org/\"):\n",
    "#         return doi\n",
    "#     if doi.startswith(\"doi:\"):\n",
    "#         doi = doi[4:]\n",
    "#     if doi.startswith(\"10.\"):\n",
    "#         return \"https://doi.org/\" + doi\n",
    "#     return doi\n",
    "\n",
    "# def extract_references(text):\n",
    "#     dois = re.findall(r'\\b10\\.\\d{4,9}/[-._;()/:a-z0-9]+', text, flags=re.I)\n",
    "#     accessions = re.findall(r'\\b(GSE\\d+|E-[A-Z]+-\\d+|PRJ[EDNA]\\d+|PDB\\s*\\w+)\\b', text, flags=re.I)\n",
    "#     references = set()\n",
    "#     for doi in dois:\n",
    "#         doi_full = normalize_doi(doi)\n",
    "#         references.add(doi_full)\n",
    "#     for acc in accessions:\n",
    "#         acc_clean = acc.replace(\" \", \"\").upper()\n",
    "#         references.add(acc_clean)\n",
    "#     return references\n",
    "\n",
    "# # === Inference ===\n",
    "# print(\"ðŸ” Running inference on test set...\")\n",
    "# submission_rows = []\n",
    "\n",
    "# for fname in os.listdir(TEST_XML_DIR):\n",
    "#     if not fname.endswith(\".xml\"):\n",
    "#         continue\n",
    "#     article_id = fname.replace(\".xml\", \"\")\n",
    "#     text = extract_text(article_id)\n",
    "#     if not text:\n",
    "#         continue\n",
    "\n",
    "#     refs = extract_references(text)\n",
    "#     for dataset_id in refs:\n",
    "#         match = re.search(re.escape(dataset_id), text, flags=re.I)\n",
    "#         if match:\n",
    "#             snippet = text[max(match.start() - 100, 0):match.end() + 100].replace(\"\\n\", \" \")\n",
    "#         else:\n",
    "#             snippet = text[:150]\n",
    "\n",
    "#         embedding = bert.encode([snippet])\n",
    "#         prob = clf.predict_proba(embedding)[0, 1]\n",
    "#         citation_type = \"Primary\" if prob >= BEST_THRESH else \"Secondary\"\n",
    "\n",
    "#         submission_rows.append({\n",
    "#             \"article_id\": article_id,\n",
    "#             \"dataset_id\": dataset_id,\n",
    "#             \"type\": citation_type\n",
    "#         })\n",
    "\n",
    "# # === Save Submission ===\n",
    "# submission_df = pd.DataFrame(submission_rows).drop_duplicates()\n",
    "# submission_df.insert(0, \"row_id\", range(len(submission_df)))\n",
    "# submission_df.to_csv(SUBMISSION_FILE, index=False)\n",
    "# print(\"âœ… Submission file created as submission.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 12656064,
     "sourceId": 82370,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 31040,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 38.895222,
   "end_time": "2025-07-05T11:40:46.344124",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-07-05T11:40:07.448902",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
